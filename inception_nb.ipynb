{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-08T00:51:16.522159Z",
     "start_time": "2017-08-08T00:51:16.502760Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x10057b550>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-08T00:51:27.123189Z",
     "start_time": "2017-08-08T00:51:26.650547Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roshkhadka/anaconda/lib/python2.7/site-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['Normalize', 'random']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import pandas\n",
    "import datetime as dt\n",
    "\n",
    "from bigdl.nn.layer import *\n",
    "from optparse import OptionParser\n",
    "from bigdl.nn.criterion import *\n",
    "from bigdl.optim.optimizer import *\n",
    "from bigdl.util.common import *\n",
    "from bigdl.dataset.transformer import *\n",
    "from bigdl.nn.initialization_method import *\n",
    "from transformer import *\n",
    "from imagenet import *\n",
    "#from bigdl.dataset import imagenet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-08T00:51:28.261538Z",
     "start_time": "2017-08-08T00:51:28.256246Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from bigdl.dataset import imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-08T00:51:28.963430Z",
     "start_time": "2017-08-08T00:51:28.956128Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scala_T(input_T):\n",
    "    if type(input_T) is list:\n",
    "        # insert into index 0 spot, such that the real data starts from index 1\n",
    "        temp = [0]\n",
    "        temp.extend(input_T)\n",
    "        return dict(enumerate(temp))\n",
    "    # if dictionary, return it back\n",
    "    return input_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-08T00:51:29.723136Z",
     "start_time": "2017-08-08T00:51:29.690645Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Inception_Layer_v1(input_size, config, name_prefix=\"\"):\n",
    "    concat = Concat(2)\n",
    "    conv1 = Sequential()\n",
    "    conv1.add(\n",
    "        SpatialConvolution(input_size,\n",
    "                           config[1][1],\n",
    "                           1, 1, 1, 1)\n",
    "            .set_name(name_prefix + \"1x1\"))\n",
    "    conv1.add(ReLU(True).set_name(name_prefix + \"relu_1x1\"))\n",
    "    concat.add(conv1)\n",
    "    conv3 = Sequential()\n",
    "    conv3.add(SpatialConvolution(input_size, config[2][1], 1, 1, 1, 1)\n",
    "              .set_name(name_prefix + \"3x3_reduce\"))\n",
    "    conv3.add(ReLU(True).set_name(name_prefix + \"relu_3x3_reduce\"))\n",
    "    conv3.add(SpatialConvolution(config[2][1], config[2][2],\n",
    "                                 3, 3, 1, 1, 1, 1)\n",
    "              .set_name(name_prefix + \"3x3\"))\n",
    "    conv3.add(ReLU(True).set_name(name_prefix + \"relu_3x3\"))\n",
    "    concat.add(conv3)\n",
    "    conv5 = Sequential()\n",
    "    conv5.add(SpatialConvolution(input_size,\n",
    "                                 config[3][1], 1, 1, 1, 1)\n",
    "              .set_name(name_prefix + \"5x5_reduce\"))\n",
    "    conv5.add(ReLU(True).set_name(name_prefix + \"relu_5x5_reduce\"))\n",
    "    conv5.add(SpatialConvolution(config[3][1],\n",
    "                                 config[3][2], 5, 5, 1, 1, 2, 2)\n",
    "              .set_name(name_prefix + \"5x5\"))\n",
    "    conv5.add(ReLU(True).set_name(name_prefix + \"relu_5x5\"))\n",
    "    concat.add(conv5)\n",
    "    pool = Sequential()\n",
    "    pool.add(SpatialMaxPooling(3, 3, 1, 1, 1, 1,\n",
    "                               to_ceil=True).set_name(name_prefix + \"pool\"))\n",
    "    pool.add(SpatialConvolution(input_size, config[4][1], 1, 1, 1, 1)\n",
    "             .set_name(name_prefix + \"pool_proj\"))\n",
    "    pool.add(ReLU(True).set_name(name_prefix + \"relu_pool_proj\"))\n",
    "    concat.add(pool).set_name(name_prefix + \"output\")\n",
    "    return concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-08T00:51:30.619283Z",
     "start_time": "2017-08-08T00:51:30.569179Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Inception_v1_NoAuxClassifier(class_num):\n",
    "    model = Sequential()\n",
    "    model.add(SpatialConvolution(3, 64, 7, 7, 2, 2, 3, 3, 1,\n",
    "                                 False).set_name(\"conv1/7x7_s2\"))\n",
    "    #---- correct API call---\n",
    "    #model.add(SpatialConvolution(3, 64, 7, 7, 2, 2, 3, 3, 1,\n",
    "    #                             False).set_init_method(weight_init_method = Xavier()).set_name(\"conv1/7x7_s2\"))\n",
    "    model.add(ReLU(True).set_name(\"conv1/relu_7x7\"))\n",
    "    model.add(SpatialMaxPooling(3, 3, 2, 2, to_ceil=True).set_name(\"pool1/3x3_s2\"))\n",
    "    model.add(SpatialCrossMapLRN(5, 0.0001, 0.75).set_name(\"pool1/norm1\"))\n",
    "    model.add(SpatialConvolution(64, 64, 1, 1, 1, 1)\n",
    "              .set_name(\"conv2/3x3_reduce\"))\n",
    "    model.add(ReLU(True).set_name(\"conv2/relu_3x3_reduce\"))\n",
    "    model.add(SpatialConvolution(64, 192, 3, 3, 1, 1, 1, 1)\n",
    "              .set_name(\"conv2/3x3\"))\n",
    "    model.add(ReLU(True).set_name(\"conv2/relu_3x3\"))\n",
    "    model.add(SpatialCrossMapLRN(5, 0.0001, 0.75).set_name(\"conv2/norm2\"))\n",
    "    model.add(SpatialMaxPooling(3, 3, 2, 2, to_ceil=True).set_name(\"pool2/3x3_s2\"))\n",
    "    model.add(Inception_Layer_v1(192, scala_T([scala_T([64]), scala_T(\n",
    "        [96, 128]), scala_T([16, 32]), scala_T([32])]), \"inception_3a/\"))\n",
    "    model.add(Inception_Layer_v1(256, scala_T([scala_T([128]), scala_T(\n",
    "        [128, 192]), scala_T([32, 96]), scala_T([64])]), \"inception_3b/\"))\n",
    "    model.add(SpatialMaxPooling(3, 3, 2, 2, to_ceil=True))\n",
    "    model.add(Inception_Layer_v1(480, scala_T([scala_T([192]), scala_T(\n",
    "        [96, 208]), scala_T([16, 48]), scala_T([64])]), \"inception_4a/\"))\n",
    "    model.add(Inception_Layer_v1(512, scala_T([scala_T([160]), scala_T(\n",
    "        [112, 224]), scala_T([24, 64]), scala_T([64])]), \"inception_4b/\"))\n",
    "    model.add(Inception_Layer_v1(512, scala_T([scala_T([128]), scala_T(\n",
    "        [128, 256]), scala_T([24, 64]), scala_T([64])]), \"inception_4c/\"))\n",
    "    model.add(Inception_Layer_v1(512, scala_T([scala_T([112]), scala_T(\n",
    "        [144, 288]), scala_T([32, 64]), scala_T([64])]), \"inception_4d/\"))\n",
    "    model.add(Inception_Layer_v1(528, scala_T([scala_T([256]), scala_T(\n",
    "        [160, 320]), scala_T([32, 128]), scala_T([128])]), \"inception_4e/\"))\n",
    "    model.add(SpatialMaxPooling(3, 3, 2, 2, to_ceil=True))\n",
    "    model.add(Inception_Layer_v1(832, scala_T([scala_T([256]), scala_T(\n",
    "        [160, 320]), scala_T([32, 128]), scala_T([128])]), \"inception_5a/\"))\n",
    "    model.add(Inception_Layer_v1(832, scala_T([scala_T([384]), scala_T(\n",
    "        [192, 384]), scala_T([48, 128]), scala_T([128])]), \"inception_5b/\"))\n",
    "    model.add(SpatialAveragePooling(7, 7, 1, 1).set_name(\"pool5/7x7_s1\"))\n",
    "    model.add(Dropout(0.4).set_name(\"pool5/drop_7x7_s1\"))\n",
    "    model.add(View([1024], num_input_dims=3))\n",
    "    model.add(Linear(1024, class_num).set_name(\"loss3/classifier\"))\n",
    "    model.add(LogSoftMax().set_name(\"loss3/loss3\"))\n",
    "    model.reset()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-08T00:51:31.645965Z",
     "start_time": "2017-08-08T00:51:31.447399Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Inception_v1(class_num):\n",
    "    feature1 = Sequential()\n",
    "    feature1.add(SpatialConvolution(3, 64, 7, 7, 2, 2, 3, 3, 1, False)\n",
    "                 .set_name(\"conv1/7x7_s2\"))\n",
    "    feature1.add(ReLU(True).set_name(\"conv1/relu_7x7\"))\n",
    "    feature1.add(\n",
    "        SpatialMaxPooling(3, 3, 2, 2, to_ceil=True)\n",
    "            .set_name(\"pool1/3x3_s2\"))\n",
    "    feature1.add(SpatialCrossMapLRN(5, 0.0001, 0.75)\n",
    "                 .set_name(\"pool1/norm1\"))\n",
    "    feature1.add(SpatialConvolution(64, 64, 1, 1, 1, 1)\n",
    "                 .set_name(\"conv2/3x3_reduce\"))\n",
    "    feature1.add(ReLU(True).set_name(\"conv2/relu_3x3_reduce\"))\n",
    "    feature1.add(SpatialConvolution(64, 192, 3, 3, 1, 1, 1, 1).set_name(\"conv2/3x3\"))\n",
    "    \n",
    "    feature1.add(ReLU(True).set_name(\"conv2/relu_3x3\"))\n",
    "    feature1.add(SpatialCrossMapLRN(5, 0.0001, 0.75).set_name(\"conv2/norm2\"))\n",
    "    feature1.add(\n",
    "        SpatialMaxPooling(3, 3, 2, 2, to_ceil=True).set_name(\"pool2/3x3_s2\"))\n",
    "    feature1.add(Inception_Layer_v1(192,\n",
    "                                    scala_T([scala_T([64]), scala_T([96, 128]),\n",
    "                                             scala_T([16, 32]), scala_T([32])]),\n",
    "                                    \"inception_3a/\"))\n",
    "    feature1.add(Inception_Layer_v1(256, scala_T([\n",
    "        scala_T([128]), scala_T([128, 192]), scala_T([32, 96]), scala_T([64])]),\n",
    "                                    \"inception_3b/\"))\n",
    "    feature1.add(\n",
    "        SpatialMaxPooling(3, 3, 2, 2, to_ceil=True).set_name(\"pool3/3x3_s2\"))\n",
    "    feature1.add(Inception_Layer_v1(480, scala_T([\n",
    "        scala_T([192]), scala_T([96, 208]), scala_T([16, 48]), scala_T([64])]),\n",
    "                                    \"inception_4a/\"))\n",
    "\n",
    "    output1 = Sequential()\n",
    "    output1.add(SpatialAveragePooling(\n",
    "        5, 5, 3, 3, ceil_mode=True).set_name(\"loss1/ave_pool\"))\n",
    "    output1.add(\n",
    "        SpatialConvolution(512, 128, 1, 1, 1, 1).set_name(\"loss1/conv\"))\n",
    "    output1.add(ReLU(True).set_name(\"loss1/relu_conv\"))\n",
    "    output1.add(View([128 * 4 * 4, 3]))\n",
    "    output1.add(Linear(128 * 4 * 4, 1024).set_name(\"loss1/fc\"))\n",
    "    output1.add(ReLU(True).set_name(\"loss1/relu_fc\"))\n",
    "    output1.add(Dropout(0.7).set_name(\"loss1/drop_fc\"))\n",
    "    output1.add(Linear(1024, class_num).set_name(\"loss1/classifier\"))\n",
    "    output1.add(LogSoftMax().set_name(\"loss1/loss\"))\n",
    "\n",
    "    feature2 = Sequential()\n",
    "    feature2.add(Inception_Layer_v1(512,\n",
    "                                    scala_T([scala_T([160]), scala_T([112, 224]),\n",
    "                                             scala_T([24, 64]), scala_T([64])]),\n",
    "                                    \"inception_4b/\"))\n",
    "    feature2.add(Inception_Layer_v1(512,\n",
    "                                    scala_T([scala_T([128]), scala_T([128, 256]),\n",
    "                                             scala_T([24, 64]), scala_T([64])]),\n",
    "                                    \"inception_4c/\"))\n",
    "    feature2.add(Inception_Layer_v1(512,\n",
    "                                    scala_T([scala_T([112]), scala_T([144, 288]),\n",
    "                                             scala_T([32, 64]), scala_T([64])]),\n",
    "                                    \"inception_4d/\"))\n",
    "\n",
    "    output2 = Sequential()\n",
    "    output2.add(SpatialAveragePooling(5, 5, 3, 3).set_name(\"loss2/ave_pool\"))\n",
    "    output2.add(\n",
    "        SpatialConvolution(528, 128, 1, 1, 1, 1).set_name(\"loss2/conv\"))\n",
    "    output2.add(ReLU(True).set_name(\"loss2/relu_conv\"))\n",
    "    output2.add(View([128 * 4 * 4, 3]))\n",
    "    output2.add(Linear(128 * 4 * 4, 1024).set_name(\"loss2/fc\"))\n",
    "    output2.add(ReLU(True).set_name(\"loss2/relu_fc\"))\n",
    "    output2.add(Dropout(0.7).set_name(\"loss2/drop_fc\"))\n",
    "    output2.add(Linear(1024, class_num).set_name(\"loss2/classifier\"))\n",
    "    output2.add(LogSoftMax().set_name(\"loss2/loss\"))\n",
    "\n",
    "    output3 = Sequential()\n",
    "    output3.add(Inception_Layer_v1(528,\n",
    "                                   scala_T([scala_T([256]), scala_T([160, 320]),\n",
    "                                            scala_T([32, 128]), scala_T([128])]),\n",
    "                                   \"inception_4e/\"))\n",
    "    output3.add(SpatialMaxPooling(3, 3, 2, 2, to_ceil=True).set_name(\"pool4/3x3_s2\"))\n",
    "    output3.add(Inception_Layer_v1(832,\n",
    "                                   scala_T([scala_T([256]), scala_T([160, 320]),\n",
    "                                            scala_T([32, 128]), scala_T([128])]),\n",
    "                                   \"inception_5a/\"))\n",
    "    output3.add(Inception_Layer_v1(832,\n",
    "                                   scala_T([scala_T([384]), scala_T([192, 384]),\n",
    "                                            scala_T([48, 128]), scala_T([128])]),\n",
    "                                   \"inception_5b/\"))\n",
    "    output3.add(SpatialAveragePooling(7, 7, 1, 1).set_name(\"pool5/7x7_s1\"))\n",
    "    output3.add(Dropout(0.4).set_name(\"pool5/drop_7x7_s1\"))\n",
    "    output3.add(View([1024, 3]))\n",
    "    output3.add(Linear(1024, class_num)\n",
    "                .set_name(\"loss3/classifier\"))\n",
    "    output3.add(LogSoftMax().set_name(\"loss3/loss3\"))\n",
    "\n",
    "    split2 = Concat(2).set_name(\"split2\")\n",
    "    split2.add(output3)\n",
    "    split2.add(output2)\n",
    "\n",
    "    mainBranch = Sequential()\n",
    "    mainBranch.add(feature2)\n",
    "    mainBranch.add(split2)\n",
    "\n",
    "    split1 = Concat(2).set_name(\"split1\")\n",
    "    split1.add(mainBranch)\n",
    "    split1.add(output1)\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(feature1)\n",
    "    model.add(split1)\n",
    "\n",
    "    model.reset()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-08T00:51:32.525906Z",
     "start_time": "2017-08-08T00:51:32.516891Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_inception_data(folder, file_type=\"image\", data_type=\"train\", normalize=255.0):\n",
    "    path = os.path.join(folder, data_type)\n",
    "    if \"seq\" == file_type:\n",
    "        # return imagenet.read_seq_file(sc, path, normalize)\n",
    "        return read_seq_file(sc, path, normalize)\n",
    "    elif \"image\" == file_type:\n",
    "        #return imagenet.read_local(sc, path, normalize)\n",
    "        return read_local(sc, path, normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-08T00:51:33.507215Z",
     "start_time": "2017-08-08T00:51:33.484918Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def config_option_parser():\n",
    "    parser = OptionParser()\n",
    "    \n",
    "    parser.add_option(\"-a\", \"--action\", dest=\"action\", default=\"train\")\n",
    "    #parser.add_option(\"-f\", \"--folder\", type=str, dest=\"folder\", default=\"\",\n",
    "    #                  help=\"url of hdfs folder store the hadoop sequence files\")\n",
    "    \n",
    "    parser.add_option(\"-f\", \"--folder\", type=str, dest=\"folder\", default=\"/home/rocky/git/datasets/notfruit/fruit_dataset/sample\",\n",
    "                      help=\"url of hdfs folder store the hadoop sequence files\")\n",
    "                        \n",
    "    parser.add_option(\"--model\", type=str, dest=\"model\", default=\"\", help=\"model snapshot location\")\n",
    "    parser.add_option(\"--state\", type=str, dest=\"state\", default=\"\", help=\"state snapshot location\")\n",
    "    parser.add_option(\"--checkpoint\", type=str, dest=\"checkpoint\", default=\"\", help=\"where to cache the model\")\n",
    "    parser.add_option(\"-o\", \"--overwrite\", action=\"store_true\", dest=\"overwrite\", default=False,\n",
    "                      help=\"overwrite checkpoint files\")\n",
    "    parser.add_option(\"-e\", \"--maxEpoch\", type=int, dest=\"maxEpoch\", default=0, help=\"epoch numbers\")\n",
    "    parser.add_option(\"-i\", \"--maxIteration\", type=int, dest=\"maxIteration\", default=62000, help=\"iteration numbers\")\n",
    "    parser.add_option(\"-l\", \"--learningRate\", type=float, dest=\"learningRate\", default=0.01, help=\"iteration numbers\")\n",
    "    parser.add_option(\"-b\", \"--batchSize\", type=int, dest=\"batchSize\",default = 64, help=\"batch size\")\n",
    "    parser.add_option(\"--classNum\", type=int, dest=\"classNum\", default=1000, help=\"class number\")\n",
    "    parser.add_option(\"--weightDecay\", type=float, dest=\"weightDecay\", default=0.0001, help=\"weight decay\")\n",
    "    parser.add_option(\"--checkpointIteration\", type=int, dest=\"checkpointIteration\", default=620,\n",
    "                      help=\"checkpoint interval of iterations\")\n",
    "\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-08T00:53:05.757807Z",
     "start_time": "2017-08-08T00:53:05.727900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createSpatialCrossMapLRN\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialCrossMapLRN\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createConcat\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createConcat\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createConcat\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialAveragePooling\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createView\n",
      "creating: createLinear\n",
      "creating: createReLU\n",
      "creating: createDropout\n",
      "creating: createLinear\n",
      "creating: createLogSoftMax\n",
      "creating: createSequential\n",
      "creating: createConcat\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createConcat\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createConcat\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialAveragePooling\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createView\n",
      "creating: createLinear\n",
      "creating: createReLU\n",
      "creating: createDropout\n",
      "creating: createLinear\n",
      "creating: createLogSoftMax\n",
      "creating: createSequential\n",
      "creating: createConcat\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createConcat\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createConcat\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialAveragePooling\n",
      "creating: createDropout\n",
      "creating: createView\n",
      "creating: createLinear\n",
      "creating: createLogSoftMax\n",
      "creating: createConcat\n",
      "creating: createSequential\n",
      "creating: createConcat\n",
      "creating: createSequential\n",
      "creating: createSeveralIteration\n",
      "creating: createSeveralIteration\n",
      "creating: createMaxIteration\n",
      "creating: createAdam\n",
      "creating: createClassNLLCriterion\n",
      "creating: createOptimizer\n",
      "creating: createEveryEpoch\n",
      "creating: createTop1Accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/roshkhadka/Desktop/BigdL_0.2.0/setup/spark-2.1.0-bin-hadoop2.7.3/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 883, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/roshkhadka/Desktop/BigdL_0.2.0/setup/spark-2.1.0-bin-hadoop2.7.3/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1040, in send_command\n",
      "    \"Error while receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "Py4JNetworkError: Error while receiving\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/roshkhadka/anaconda/lib/python2.7/SocketServer.py\", line 290, in _handle_request_noblock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 60227)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    self.process_request(request, client_address)\n",
      "  File \"/Users/roshkhadka/anaconda/lib/python2.7/SocketServer.py\", line 318, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/Users/roshkhadka/anaconda/lib/python2.7/SocketServer.py\", line 331, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/Users/roshkhadka/anaconda/lib/python2.7/SocketServer.py\", line 652, in __init__\n",
      "    self.handle()\n",
      "  File \"/Users/roshkhadka/Desktop/BigdL_0.2.0/setup/spark-2.1.0-bin-hadoop2.7.3/python/pyspark/accumulators.py\", line 235, in handle\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/roshkhadka/Desktop/BigdL_0.2.0/setup/spark-2.1.0-bin-hadoop2.7.3/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 883, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/roshkhadka/Desktop/BigdL_0.2.0/setup/spark-2.1.0-bin-hadoop2.7.3/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1040, in send_command\n",
      "    \"Error while receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "Py4JNetworkError: Error while receiving\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/Users/roshkhadka/Desktop/BigdL_0.2.0/setup/spark-2.1.0-bin-hadoop2.7.3/python/pyspark/serializers.py\", line 557, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling None.None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/Users/roshkhadka/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result)\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2897\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_in_exec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2898\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2899\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2900\u001b[0m             \u001b[0moutflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/roshkhadka/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only)\u001b[0m\n\u001b[1;32m   1824\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   1825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1826\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1827\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_pdb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1828\u001b[0m                         \u001b[0;31m# drop into debugger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/roshkhadka/anaconda/lib/python2.7/site-packages/ipykernel/zmqshell.pyc\u001b[0m in \u001b[0;36m_showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    509\u001b[0m             \u001b[0;34mu'traceback'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;34mu'ename'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0municode_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m             \u001b[0;34mu'evalue'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mpy3compat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msafe_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m         }\n\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/roshkhadka/anaconda/lib/python2.7/site-packages/ipython_genutils/py3compat.pyc\u001b[0m in \u001b[0;36msafe_unicode\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \"\"\"\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0municode_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mUnicodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/roshkhadka/Desktop/BigdL_0.2.0/setup/spark-2.1.0-bin-hadoop2.7.3/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36m__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0mgateway_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception_cmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m         \u001b[0mreturn_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_return_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"{0}: {1}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/roshkhadka/Desktop/BigdL_0.2.0/setup/spark-2.1.0-bin-hadoop2.7.3/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m             raise Py4JError(\n\u001b[1;32m    326\u001b[0m                 \u001b[0;34m\"An error occurred while calling {0}{1}{2}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 format(target_id, \".\", name))\n\u001b[0m\u001b[1;32m    328\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0mtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling None.None"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":     \n",
    "    # parse options\n",
    "\n",
    "    parser = config_option_parser()\n",
    "    (options, args) = parser.parse_args(sys.argv)\n",
    "\n",
    "    if not options.learningRate:\n",
    "        parser.error(\"-l --learningRate is a mandatory opt\")\n",
    "    if not options.batchSize:\n",
    "        parser.error(\"-b --batchSize is a mandatory opt\")\n",
    "        \n",
    "    #learningRate = 0.001    imagenet\n",
    "\n",
    "    # init\n",
    "    #sc = SparkContext(appName=\"inception v1\", conf=create_spark_conf())\n",
    "    init_engine()\n",
    "    # build model\n",
    "#     inception_model = Inception_v1_NoAuxClassifier(options.classNum)\n",
    "    inception_model = Inception_v1(options.classNum)\n",
    "\n",
    "    image_size = 224\n",
    "    options.action = \"train\"\n",
    "    if options.action == \"train\":\n",
    "        # create dataset\n",
    "        train_transformer = Transformer([Crop(image_size, image_size),\n",
    "                                         Flip(0.5),\n",
    "                                         ChannelNormalizer(0.485, 0.456, 0.406, 0.229, 0.224, 0.225),\n",
    "                                         TransposeToTensor(False)\n",
    "                                         ])\n",
    "        #train_data = get_inception_data(options.folder, \"seq\", \"train\").map(\n",
    "        #options.folder = \"/home/rocky/git/datasets/notfruit/fruit_dataset/sample/\"\n",
    "        options.folder = \"./sample\"\n",
    "\n",
    "        train_data = get_inception_data(options.folder, \"image\", \"train\").map(\n",
    "            lambda features_label: (train_transformer(features_label[0]), features_label[1])).map(\n",
    "            lambda features_label: Sample.from_ndarray(features_label[0], features_label[1] + 1))\n",
    "\n",
    "        val_transformer = Transformer([Crop(image_size, image_size, \"center\"),\n",
    "                                       Flip(0.5),\n",
    "                                       ChannelNormalizer(0.485, 0.456, 0.406, 0.229, 0.224, 0.225),\n",
    "                                       TransposeToTensor(False)\n",
    "                                       ])\n",
    "        #val_data = get_inception_data(options.folder, \"seq\", \"val\").map(\n",
    "        val_data = get_inception_data(options.folder, \"image\", \"val\").map(\n",
    "            lambda features_label: (val_transformer(features_label[0]), features_label[1])).map(\n",
    "            lambda features_label: Sample.from_ndarray(features_label[0], features_label[1] + 1))\n",
    "\n",
    "        # TODO: Check stateSnapshot opt\n",
    "\n",
    "        if options.maxEpoch:\n",
    "                {\"learningRate\": options.learningRate,\n",
    "                 \"weightDecay\": options.weightDecay,\n",
    "                 \"momentum\": 0.9,\n",
    "                 \"dampening\": 0.0}\n",
    "                checkpoint_trigger = EveryEpoch()\n",
    "                test_trigger = EveryEpoch()\n",
    "                end_trigger = MaxEpoch(options.maxEpoch)\n",
    "        else:\n",
    "            state = scala_T(\n",
    "                {\"learningRate\": options.learningRate,\n",
    "                 \"weightDecay\": options.weightDecay,\n",
    "                 \"momentum\": 0.9,\n",
    "                 \"dampening\": 0.0})\n",
    "            checkpoint_trigger = SeveralIteration(options.checkpointIteration)\n",
    "            test_trigger = SeveralIteration(options.checkpointIteration)\n",
    "            end_trigger = MaxIteration(options.maxIteration)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = Optimizer(\n",
    "            model=inception_model,\n",
    "            training_rdd=train_data,\n",
    "            #optim_method=\"SGD\",\n",
    "            #optim_method=SGD(learningrate=0.01, learningrate_decay=0.0002)\n",
    "            optim_method=Adam(learningrate=0.002),\n",
    "            criterion=ClassNLLCriterion(),\n",
    "            end_trigger=end_trigger,\n",
    "            batch_size=options.batchSize,\n",
    "            #state=state\n",
    "        )\n",
    "        options.batchSize = 64 \n",
    "        if options.checkpoint:\n",
    "            optimizer.set_checkpoint(checkpoint_trigger, options.checkpoint, options.overwrite)\n",
    "\n",
    "        \"\"\"\n",
    "        optimizer.set_validation(trigger=test_trigger,\n",
    "                                 val_rdd=val_data,\n",
    "                                 batch_size=options.batchSize,\n",
    "                                 #val_method=[\"Top1Accuracy\", \"Top5Accuracy\"])\n",
    "                                 val_method=[\"Loss\"]\n",
    "                                 val_method=[Top1Accuracy()])\n",
    "        \"\"\"\n",
    "        optimizer.set_validation(batch_size=2048,\n",
    "                                val_rdd=val_data,\n",
    "                                trigger=EveryEpoch(),\n",
    "                                val_method=[Top1Accuracy()])\n",
    "\n",
    "        trained_model = optimizer.optimize()\n",
    "\n",
    "\n",
    "    options.action = \"test\"\n",
    "    if options.action == \"test\":\n",
    "        # Load a pre-trained model and then validate it through top1 accuracy.\n",
    "        test_transformer = Transformer([Crop(image_size, image_size, \"center\"),\n",
    "                                        Flip(0.5),\n",
    "                                        ChannelNormalizer(0.485, 0.456, 0.406, 0.229, 0.224, 0.225),\n",
    "                                        TransposeToTensor(False)\n",
    "                                        ])\n",
    "        #test_data = get_inception_data(options.folder, \"\", \"val\").map(\n",
    "        test_data = get_inception_data(options.folder, \"image\", \"val\").map(\n",
    "            lambda features_label: (test_transformer(features_label[0]), features_label[1])).map(\n",
    "            lambda features_label: Sample.from_ndarray(features_label[0], features_label[1] + 1))\n",
    "        model = Model.load(options.model)\n",
    "        results = model.test(test_data, options.batchSize, [\"Top1Accuracy\", \"Top5Accuracy\"])\n",
    "        for result in results:\n",
    "            print result\n",
    "\n",
    "    print (\"---- the end -----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'hello'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
