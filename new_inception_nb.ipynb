{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x10077b550>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-09T16:49:06.328360Z",
     "start_time": "2017-08-09T16:49:04.454795Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import pandas\n",
    "import datetime as dt\n",
    "\n",
    "from bigdl.nn.layer import *\n",
    "from optparse import OptionParser\n",
    "from bigdl.nn.criterion import *\n",
    "from bigdl.optim.optimizer import *\n",
    "from bigdl.util.common import *\n",
    "from bigdl.dataset.transformer import *\n",
    "from bigdl.nn.initialization_method import *\n",
    "from transformer import *\n",
    "from imagenet import *\n",
    "#from bigdl.dataset import imagenet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-09T16:49:06.355969Z",
     "start_time": "2017-08-09T16:49:06.352181Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from bigdl.dataset import imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-09T16:49:13.468066Z",
     "start_time": "2017-08-09T16:49:13.456193Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scala_T(input_T):\n",
    "    if type(input_T) is list:\n",
    "        # insert into index 0 spot, such that the real data starts from index 1\n",
    "        temp = [0]\n",
    "        temp.extend(input_T)\n",
    "        return dict(enumerate(temp))\n",
    "    # if dictionary, return it back\n",
    "    return input_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-09T16:49:16.362249Z",
     "start_time": "2017-08-09T16:49:16.334382Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Inception_Layer_v1(input_size, config, name_prefix=\"\"):\n",
    "    concat = Concat(2)\n",
    "    conv1 = Sequential()\n",
    "    conv1.add(\n",
    "        SpatialConvolution(input_size,\n",
    "                           config[1][1],\n",
    "                           1, 1, 1, 1)\n",
    "            .set_name(name_prefix + \"1x1\"))\n",
    "    conv1.add(ReLU(True).set_name(name_prefix + \"relu_1x1\"))\n",
    "    concat.add(conv1)\n",
    "    conv3 = Sequential()\n",
    "    conv3.add(SpatialConvolution(input_size, config[2][1], 1, 1, 1, 1)\n",
    "              .set_name(name_prefix + \"3x3_reduce\"))\n",
    "    conv3.add(ReLU(True).set_name(name_prefix + \"relu_3x3_reduce\"))\n",
    "    conv3.add(SpatialConvolution(config[2][1], config[2][2],\n",
    "                                 3, 3, 1, 1, 1, 1)\n",
    "              .set_name(name_prefix + \"3x3\"))\n",
    "    conv3.add(ReLU(True).set_name(name_prefix + \"relu_3x3\"))\n",
    "    concat.add(conv3)\n",
    "    conv5 = Sequential()\n",
    "    conv5.add(SpatialConvolution(input_size,\n",
    "                                 config[3][1], 1, 1, 1, 1)\n",
    "              .set_name(name_prefix + \"5x5_reduce\"))\n",
    "    conv5.add(ReLU(True).set_name(name_prefix + \"relu_5x5_reduce\"))\n",
    "    conv5.add(SpatialConvolution(config[3][1],\n",
    "                                 config[3][2], 5, 5, 1, 1, 2, 2)\n",
    "              .set_name(name_prefix + \"5x5\"))\n",
    "    conv5.add(ReLU(True).set_name(name_prefix + \"relu_5x5\"))\n",
    "    concat.add(conv5)\n",
    "    pool = Sequential()\n",
    "    pool.add(SpatialMaxPooling(3, 3, 1, 1, 1, 1,\n",
    "                               to_ceil=True).set_name(name_prefix + \"pool\"))\n",
    "    pool.add(SpatialConvolution(input_size, config[4][1], 1, 1, 1, 1)\n",
    "             .set_name(name_prefix + \"pool_proj\"))\n",
    "    pool.add(ReLU(True).set_name(name_prefix + \"relu_pool_proj\"))\n",
    "    concat.add(pool).set_name(name_prefix + \"output\")\n",
    "    return concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-09T16:49:17.812870Z",
     "start_time": "2017-08-09T16:49:17.755210Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Inception_v1_NoAuxClassifier(class_num):\n",
    "    model = Sequential()\n",
    "    model.add(SpatialConvolution(3, 64, 7, 7, 2, 2, 3, 3, 1,\n",
    "                                 False).set_name(\"conv1/7x7_s2\"))\n",
    "    #---- correct API call---\n",
    "    #model.add(SpatialConvolution(3, 64, 7, 7, 2, 2, 3, 3, 1,\n",
    "    #                             False).set_init_method(weight_init_method = Xavier()).set_name(\"conv1/7x7_s2\"))\n",
    "    model.add(ReLU(True).set_name(\"conv1/relu_7x7\"))\n",
    "    model.add(SpatialMaxPooling(3, 3, 2, 2, to_ceil=True).set_name(\"pool1/3x3_s2\"))\n",
    "    model.add(SpatialCrossMapLRN(5, 0.0001, 0.75).set_name(\"pool1/norm1\"))\n",
    "    model.add(SpatialConvolution(64, 64, 1, 1, 1, 1)\n",
    "              .set_name(\"conv2/3x3_reduce\"))\n",
    "    model.add(ReLU(True).set_name(\"conv2/relu_3x3_reduce\"))\n",
    "    model.add(SpatialConvolution(64, 192, 3, 3, 1, 1, 1, 1)\n",
    "              .set_name(\"conv2/3x3\"))\n",
    "    model.add(ReLU(True).set_name(\"conv2/relu_3x3\"))\n",
    "    model.add(SpatialCrossMapLRN(5, 0.0001, 0.75).set_name(\"conv2/norm2\"))\n",
    "    model.add(SpatialMaxPooling(3, 3, 2, 2, to_ceil=True).set_name(\"pool2/3x3_s2\"))\n",
    "    model.add(Inception_Layer_v1(192, scala_T([scala_T([64]), scala_T(\n",
    "        [96, 128]), scala_T([16, 32]), scala_T([32])]), \"inception_3a/\"))\n",
    "    model.add(Inception_Layer_v1(256, scala_T([scala_T([128]), scala_T(\n",
    "        [128, 192]), scala_T([32, 96]), scala_T([64])]), \"inception_3b/\"))\n",
    "    model.add(SpatialMaxPooling(3, 3, 2, 2, to_ceil=True))\n",
    "    model.add(Inception_Layer_v1(480, scala_T([scala_T([192]), scala_T(\n",
    "        [96, 208]), scala_T([16, 48]), scala_T([64])]), \"inception_4a/\"))\n",
    "    model.add(Inception_Layer_v1(512, scala_T([scala_T([160]), scala_T(\n",
    "        [112, 224]), scala_T([24, 64]), scala_T([64])]), \"inception_4b/\"))\n",
    "    model.add(Inception_Layer_v1(512, scala_T([scala_T([128]), scala_T(\n",
    "        [128, 256]), scala_T([24, 64]), scala_T([64])]), \"inception_4c/\"))\n",
    "    model.add(Inception_Layer_v1(512, scala_T([scala_T([112]), scala_T(\n",
    "        [144, 288]), scala_T([32, 64]), scala_T([64])]), \"inception_4d/\"))\n",
    "    model.add(Inception_Layer_v1(528, scala_T([scala_T([256]), scala_T(\n",
    "        [160, 320]), scala_T([32, 128]), scala_T([128])]), \"inception_4e/\"))\n",
    "    model.add(SpatialMaxPooling(3, 3, 2, 2, to_ceil=True))\n",
    "    model.add(Inception_Layer_v1(832, scala_T([scala_T([256]), scala_T(\n",
    "        [160, 320]), scala_T([32, 128]), scala_T([128])]), \"inception_5a/\"))\n",
    "    model.add(Inception_Layer_v1(832, scala_T([scala_T([384]), scala_T(\n",
    "        [192, 384]), scala_T([48, 128]), scala_T([128])]), \"inception_5b/\"))\n",
    "    model.add(SpatialAveragePooling(7, 7, 1, 1).set_name(\"pool5/7x7_s1\"))\n",
    "    model.add(Dropout(0.4).set_name(\"pool5/drop_7x7_s1\"))\n",
    "    model.add(View([1024], num_input_dims=3))\n",
    "    model.add(Linear(1024, class_num).set_name(\"loss3/classifier\"))\n",
    "    model.add(LogSoftMax().set_name(\"loss3/loss3\"))\n",
    "    model.reset()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-09T16:49:18.996426Z",
     "start_time": "2017-08-09T16:49:18.792800Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Inception_v1(class_num):\n",
    "    feature1 = Sequential()\n",
    "    feature1.add(SpatialConvolution(3, 64, 7, 7, 2, 2, 3, 3, 1, False)\n",
    "                 .set_name(\"conv1/7x7_s2\"))\n",
    "    feature1.add(ReLU(True).set_name(\"conv1/relu_7x7\"))\n",
    "    feature1.add(\n",
    "        SpatialMaxPooling(3, 3, 2, 2, to_ceil=True)\n",
    "            .set_name(\"pool1/3x3_s2\"))\n",
    "    feature1.add(SpatialCrossMapLRN(5, 0.0001, 0.75)\n",
    "                 .set_name(\"pool1/norm1\"))\n",
    "    feature1.add(SpatialConvolution(64, 64, 1, 1, 1, 1)\n",
    "                 .set_name(\"conv2/3x3_reduce\"))\n",
    "    feature1.add(ReLU(True).set_name(\"conv2/relu_3x3_reduce\"))\n",
    "    feature1.add(SpatialConvolution(64, 192, 3, 3, 1,\n",
    "                                    1, 1, 1).set_name(\"conv2/3x3\"))\n",
    "    feature1.add(ReLU(True).set_name(\"conv2/relu_3x3\"))\n",
    "    feature1.add(SpatialCrossMapLRN(5, 0.0001, 0.75).set_name(\"conv2/norm2\"))\n",
    "    feature1.add(\n",
    "        SpatialMaxPooling(3, 3, 2, 2, to_ceil=True).set_name(\"pool2/3x3_s2\"))\n",
    "    feature1.add(Inception_Layer_v1(192,\n",
    "                                    scala_T([scala_T([64]), scala_T([96, 128]),\n",
    "                                             scala_T([16, 32]), scala_T([32])]),\n",
    "                                    \"inception_3a/\"))\n",
    "    feature1.add(Inception_Layer_v1(256, scala_T([\n",
    "        scala_T([128]), scala_T([128, 192]), scala_T([32, 96]), scala_T([64])]),\n",
    "                                    \"inception_3b/\"))\n",
    "    feature1.add(\n",
    "        SpatialMaxPooling(3, 3, 2, 2, to_ceil=True).set_name(\"pool3/3x3_s2\"))\n",
    "    feature1.add(Inception_Layer_v1(480, scala_T([\n",
    "        scala_T([192]), scala_T([96, 208]), scala_T([16, 48]), scala_T([64])]),\n",
    "                                    \"inception_4a/\"))\n",
    "\n",
    "    output1 = Sequential()\n",
    "    output1.add(SpatialAveragePooling(\n",
    "        5, 5, 3, 3, ceil_mode=True).set_name(\"loss1/ave_pool\"))\n",
    "    output1.add(\n",
    "        SpatialConvolution(512, 128, 1, 1, 1, 1).set_name(\"loss1/conv\"))\n",
    "    output1.add(ReLU(True).set_name(\"loss1/relu_conv\"))\n",
    "    output1.add(View([128 * 4 * 4, 3]))\n",
    "    output1.add(Linear(128 * 4 * 4, 1024).set_name(\"loss1/fc\"))\n",
    "    output1.add(ReLU(True).set_name(\"loss1/relu_fc\"))\n",
    "    output1.add(Dropout(0.7).set_name(\"loss1/drop_fc\"))\n",
    "    output1.add(Linear(1024, class_num).set_name(\"loss1/classifier\"))\n",
    "    output1.add(LogSoftMax().set_name(\"loss1/loss\"))\n",
    "\n",
    "    feature2 = Sequential()\n",
    "    feature2.add(Inception_Layer_v1(512,\n",
    "                                    scala_T([scala_T([160]), scala_T([112, 224]),\n",
    "                                             scala_T([24, 64]), scala_T([64])]),\n",
    "                                    \"inception_4b/\"))\n",
    "    feature2.add(Inception_Layer_v1(512,\n",
    "                                    scala_T([scala_T([128]), scala_T([128, 256]),\n",
    "                                             scala_T([24, 64]), scala_T([64])]),\n",
    "                                    \"inception_4c/\"))\n",
    "    feature2.add(Inception_Layer_v1(512,\n",
    "                                    scala_T([scala_T([112]), scala_T([144, 288]),\n",
    "                                             scala_T([32, 64]), scala_T([64])]),\n",
    "                                    \"inception_4d/\"))\n",
    "\n",
    "    output2 = Sequential()\n",
    "    output2.add(SpatialAveragePooling(5, 5, 3, 3).set_name(\"loss2/ave_pool\"))\n",
    "    output2.add(\n",
    "        SpatialConvolution(528, 128, 1, 1, 1, 1).set_name(\"loss2/conv\"))\n",
    "    output2.add(ReLU(True).set_name(\"loss2/relu_conv\"))\n",
    "    output2.add(View([128 * 4 * 4, 3]))\n",
    "    output2.add(Linear(128 * 4 * 4, 1024).set_name(\"loss2/fc\"))\n",
    "    output2.add(ReLU(True).set_name(\"loss2/relu_fc\"))\n",
    "    output2.add(Dropout(0.7).set_name(\"loss2/drop_fc\"))\n",
    "    output2.add(Linear(1024, class_num).set_name(\"loss2/classifier\"))\n",
    "    output2.add(LogSoftMax().set_name(\"loss2/loss\"))\n",
    "\n",
    "    output3 = Sequential()\n",
    "    output3.add(Inception_Layer_v1(528,\n",
    "                                   scala_T([scala_T([256]), scala_T([160, 320]),\n",
    "                                            scala_T([32, 128]), scala_T([128])]),\n",
    "                                   \"inception_4e/\"))\n",
    "    output3.add(SpatialMaxPooling(3, 3, 2, 2, to_ceil=True).set_name(\"pool4/3x3_s2\"))\n",
    "    output3.add(Inception_Layer_v1(832,\n",
    "                                   scala_T([scala_T([256]), scala_T([160, 320]),\n",
    "                                            scala_T([32, 128]), scala_T([128])]),\n",
    "                                   \"inception_5a/\"))\n",
    "    output3.add(Inception_Layer_v1(832,\n",
    "                                   scala_T([scala_T([384]), scala_T([192, 384]),\n",
    "                                            scala_T([48, 128]), scala_T([128])]),\n",
    "                                   \"inception_5b/\"))\n",
    "    output3.add(SpatialAveragePooling(7, 7, 1, 1).set_name(\"pool5/7x7_s1\"))\n",
    "    output3.add(Dropout(0.4).set_name(\"pool5/drop_7x7_s1\"))\n",
    "    output3.add(View([1024, 3]))\n",
    "    output3.add(Linear(1024, class_num)\n",
    "                .set_name(\"loss3/classifier\"))\n",
    "    output3.add(LogSoftMax().set_name(\"loss3/loss3\"))\n",
    "\n",
    "    split2 = Concat(2).set_name(\"split2\")\n",
    "    split2.add(output3)\n",
    "    split2.add(output2)\n",
    "\n",
    "    mainBranch = Sequential()\n",
    "    mainBranch.add(feature2)\n",
    "    mainBranch.add(split2)\n",
    "\n",
    "    split1 = Concat(2).set_name(\"split1\")\n",
    "    split1.add(mainBranch)\n",
    "    split1.add(output1)\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(feature1)\n",
    "    model.add(split1)\n",
    "\n",
    "    model.reset()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-09T16:49:20.973708Z",
     "start_time": "2017-08-09T16:49:20.965794Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_inception_data(folder, file_type=\"image\", data_type=\"train\", normalize=255.0):\n",
    "    path = os.path.join(folder, data_type)\n",
    "    if \"seq\" == file_type:\n",
    "        # return imagenet.read_seq_file(sc, path, normalize)\n",
    "        return read_seq_file(sc, path, normalize)\n",
    "    elif \"image\" == file_type:\n",
    "        #return imagenet.read_local(sc, path, normalize)\n",
    "        return read_local(sc, path, normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-09T16:49:22.729563Z",
     "start_time": "2017-08-09T16:49:22.203181Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initializing BigDL engine\n",
    "init_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-09T16:49:23.661768Z",
     "start_time": "2017-08-09T16:49:23.655313Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# paths for datasets, saving checkpoints \n",
    "\n",
    "DATA_PATH = \"./sample/\"\n",
    "checkpoint_path = \"./sample/checkpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-09T16:49:27.531962Z",
     "start_time": "2017-08-09T16:49:25.064097Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createSpatialCrossMapLRN\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialCrossMapLRN\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createConcat\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createConcat\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createConcat\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createConcat\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createConcat\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createConcat\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createConcat\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createConcat\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createConcat\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialAveragePooling\n",
      "creating: createDropout\n",
      "creating: createView\n",
      "creating: createLinear\n",
      "creating: createLogSoftMax\n"
     ]
    }
   ],
   "source": [
    "#providing the no of classes in the dataset to model\n",
    "classNum = 1000\n",
    "\n",
    "# iporting the model\n",
    "#inception_model = Inception_v1(options.classNum)  # main inception-v1 model\n",
    "inception_model = Inception_v1_NoAuxClassifier(classNum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-09T16:49:33.196765Z",
     "start_time": "2017-08-09T16:49:32.720182Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 20] Not a directory: './sample/train/.DS_Store'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-02f814c62486>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# reading the traning data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m train_data = get_inception_data(DATA_PATH, \"image\", \"train\").map(\n\u001b[0m\u001b[1;32m     17\u001b[0m                 \u001b[0;32mlambda\u001b[0m \u001b[0mfeatures_label\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_transformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 lambda features_label: Sample.from_ndarray(features_label[0], features_label[1] + 1))\n",
      "\u001b[0;32m<ipython-input-8-8b1793d019fd>\u001b[0m in \u001b[0;36mget_inception_data\u001b[0;34m(folder, file_type, data_type, normalize)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m\"image\"\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mfile_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m#return imagenet.read_local(sc, path, normalize)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mread_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/roshkhadka/Desktop/BigdL_0.2.0/imagenet.pyc\u001b[0m in \u001b[0;36mread_local\u001b[0;34m(sc, folder, normalize, has_label)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \"\"\"\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# read directory, create image paths list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mimage_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_local_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;31m# create rdd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mimage_paths_rdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/roshkhadka/Desktop/BigdL_0.2.0/imagenet.pyc\u001b[0m in \u001b[0;36mread_local_path\u001b[0;34m(folder, has_label)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mdirs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                 \u001b[0mimage_paths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 20] Not a directory: './sample/train/.DS_Store'"
     ]
    }
   ],
   "source": [
    "# reading the data and performing pre-processing \n",
    "\n",
    "# the image size expected by the model\n",
    "image_size = 224\n",
    "\n",
    "# how is the train, val data loaded ?. Are they read as a batch or whole set in one go, where can i  specify the \n",
    "# size that is loaded into my RAM\n",
    "\n",
    "# image transformer, used for pre-processing the train images \n",
    "train_transformer = Transformer([Crop(image_size, image_size),\n",
    "                                  Flip(0.5),\n",
    "                                  ChannelNormalizer(0.485, 0.456, 0.406, 0.229, 0.224, 0.225),\n",
    "                                  TransposeToTensor(False)])\n",
    "\n",
    "# reading the traning data\n",
    "train_data = get_inception_data(DATA_PATH, \"image\", \"train\").map(\n",
    "                lambda features_label: (train_transformer(features_label[0]), features_label[1])).map(\n",
    "                lambda features_label: Sample.from_ndarray(features_label[0], features_label[1] + 1))\n",
    "\n",
    "\n",
    "# why is this different from train Transformer --cropped at center --- ? \n",
    "# image transformer, used for pre-processing the validation images \n",
    "val_transformer = Transformer([Crop(image_size, image_size, \"center\"),\n",
    "                                Flip(0.5),\n",
    "                                ChannelNormalizer(0.485, 0.456, 0.406, 0.229, 0.224, 0.225),\n",
    "                                TransposeToTensor(False)])\n",
    "\n",
    "#reading the validation data\n",
    "val_data = get_inception_data(DATA_PATH, \"image\", \"val\").map(\n",
    "                lambda features_label: (val_transformer(features_label[0]), features_label[1])).map(\n",
    "                lambda features_label: Sample.from_ndarray(features_label[0], features_label[1] + 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-09T16:49:37.529Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-004a81b26e86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m optimizer = Optimizer(\n\u001b[1;32m      8\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minception_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                 \u001b[0mtraining_rdd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m                 \u001b[0moptim_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearningrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.002\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mClassNLLCriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# parameters for \n",
    "batch_size = 64\n",
    "no_epochs = 2\n",
    "\n",
    "# Optimizer\n",
    "optimizer = Optimizer(\n",
    "                model=inception_model,\n",
    "                training_rdd=train_data,\n",
    "                optim_method=Adam(learningrate=0.002),\n",
    "                criterion=ClassNLLCriterion(),\n",
    "                end_trigger=MaxEpoch(no_epochs),\n",
    "                batch_size=batch_size\n",
    "            )\n",
    "\n",
    "# setting checkpoints\n",
    "optimizer.set_checkpoint(EveryEpoch(), checkpoint_path, isOverWrite=False)\n",
    "\n",
    "# setting validation parameters \n",
    "optimizer.set_validation( batch_size=batch_size,\n",
    "                          val_rdd=val_data,\n",
    "                          trigger=EveryEpoch(),\n",
    "                          val_method=[Top1Accuracy()])\n",
    "\n",
    "# traning the model\n",
    "trained_model = optimizer.optimize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# testing our model\n",
    "\n",
    "# image transformer, used for pre-processing the validation images \n",
    "test_transformer = Transformer([Crop(image_size, image_size, \"center\"),\n",
    "                                Flip(0.5),\n",
    "                                ChannelNormalizer(0.485, 0.456, 0.406, 0.229, 0.224, 0.225),\n",
    "                                TransposeToTensor(False)])\n",
    "\n",
    "# shouldn't the option to be passed is 'test' here rather than 'val' ? \n",
    "# reading test data \n",
    "test_data = get_inception_data(DATA_PATH, \"image\", \"val\").map(\n",
    "                lambda features_label: (test_transformer(features_label[0]), features_label[1])).map(\n",
    "                lambda features_label: Sample.from_ndarray(features_label[0], features_label[1] + 1))\n",
    "\n",
    "'''\n",
    "# Incase the model is alreay built and want to test it\n",
    "# path to the saved model\n",
    "model_path = \"  \"\n",
    "model = Model.load(options.model)\n",
    "results = model.test(test_data, options.batchSize, [\"Top1Accuracy\", \"Top5Accuracy\"])\n",
    "'''\n",
    "\n",
    "# testing the trained model \n",
    "results = trained_model.test(test_data, options.batchSize, [\"Top1Accuracy\", \"Top5Accuracy\"])\n",
    "\n",
    "for result in results:\n",
    "    print result\n",
    "\n",
    "print (\"---- the end -----\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
