{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import join, basename\n",
    "import struct\n",
    "from scipy import misc\n",
    "from transformer import Resize\n",
    "\n",
    "#Import all the required packages\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "import pandas\n",
    "import datetime as dt\n",
    "\n",
    "#Importing necessary packages \n",
    "from bigdl.nn.layer import *\n",
    "from optparse import OptionParser\n",
    "from bigdl.nn.criterion import *\n",
    "from bigdl.optim.optimizer import *\n",
    "from bigdl.util.common import *\n",
    "from bigdl.dataset.transformer import *\n",
    "from bigdl.nn.initialization_method import *\n",
    "from transformer import *\n",
    "from imagenet import *\n",
    "#from bigdl.dataset import imagenet\n",
    "\n",
    "\n",
    "\n",
    "def read_local_path(folder, has_label=True):\n",
    "    # read directory, create map\n",
    "    dirs = listdir(folder)\n",
    "    print folder\n",
    "    print dirs\n",
    "    # create image path and label list\n",
    "    image_paths = []\n",
    "    print image_paths\n",
    "    if has_label:\n",
    "        dirs.sort()\n",
    "        for d in dirs:\n",
    "            for f in listdir(join(folder, d)):\n",
    "                image_paths.append((join(join(folder, d), f), dirs.index(d) + 1))\n",
    "    else:\n",
    "        for f in dirs:\n",
    "            image_paths.append((join(folder, f), -1))\n",
    "    return image_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_local(sc, folder, normalize=255.0, has_label=True):\n",
    "    \"\"\"\n",
    "    Read images from local directory\n",
    "    :param sc: spark context\n",
    "    :param folder: local directory\n",
    "    :param normalize: normalization value\n",
    "    :param has_label: whether the image folder contains label\n",
    "    :return: RDD of sample\n",
    "    \"\"\"\n",
    "    # read directory, create image paths list\n",
    "    image_paths = read_local_path(folder, has_label)\n",
    "    print image_paths\n",
    "    # create rdd\n",
    "    image_paths_rdd = sc.parallelize(image_paths)\n",
    "    print image_paths_rdd\n",
    "    feature_label_rdd = image_paths_rdd.map(lambda path_label: (misc.imread(path_label[0]), np.array(path_label[1]))) \\\n",
    "        .map(lambda img_label:\n",
    "             (Resize(256, 256)(img_label[0]), img_label[1])) \\\n",
    "        .map(lambda feature_label:\n",
    "             (((feature_label[0] & 0xff) / normalize).astype(\"float32\"), feature_label[1]))\n",
    "    return feature_label_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function takes an input, if the input is a list, it insert into index 0 spot, such that the real data starts from index 1. Returns back a dictionary with key being the index and value the list of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scala_T(input_T):\n",
    "    \n",
    "    if type(input_T) is list:\n",
    "        # insert into index 0 spot, such that the real data starts from index 1\n",
    "        temp = [0]\n",
    "        temp.extend(input_T)\n",
    "        return dict(enumerate(temp))\n",
    "    # if dictionary, return it back\n",
    "    return input_T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following codes basically creates functions that initiate the inception model. Each function is independent of the other, the architecture is what makes these functions unique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Inception_Layer_v1(input_size, config, name_prefix=\"\"):\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Concat is a container who concatenates the output of it's submodules along the provided dimension: all submodules \n",
    "    take the same inputs, and their output is concatenated.\n",
    "    '''\n",
    "    concat = Concat(2)\n",
    "    \n",
    "    \"\"\"\n",
    "    In the above code, we first create a container Sequential. Then add the layers into the container one by one. The \n",
    "    order of the layers in the model is same with the insertion order. \n",
    "    \n",
    "    \"\"\"\n",
    "    conv1 = Sequential()\n",
    "    \n",
    "    #Adding layes to the conv1 model we jus created\n",
    "    \n",
    "    #SpatialConvolution is a module that applies a 2D convolution over an input image.\n",
    "    conv1.add(SpatialConvolution(input_size, config[1][1], 1, 1, 1, 1).set_name(name_prefix + \"1x1\"))\n",
    "    conv1.add(ReLU(True).set_name(name_prefix + \"relu_1x1\"))\n",
    "    concat.add(conv1)\n",
    "    \n",
    "    conv3 = Sequential()\n",
    "    conv3.add(SpatialConvolution(input_size, config[2][1], 1, 1, 1, 1).set_name(name_prefix + \"3x3_reduce\"))\n",
    "    conv3.add(ReLU(True).set_name(name_prefix + \"relu_3x3_reduce\"))\n",
    "    conv3.add(SpatialConvolution(config[2][1], config[2][2], 3, 3, 1, 1, 1, 1).set_name(name_prefix + \"3x3\"))\n",
    "    conv3.add(ReLU(True).set_name(name_prefix + \"relu_3x3\"))\n",
    "    concat.add(conv3)\n",
    "    \n",
    "    \n",
    "    conv5 = Sequential()\n",
    "    conv5.add(SpatialConvolution(input_size,config[3][1], 1, 1, 1, 1).set_name(name_prefix + \"5x5_reduce\"))\n",
    "    conv5.add(ReLU(True).set_name(name_prefix + \"relu_5x5_reduce\"))\n",
    "    conv5.add(SpatialConvolution(config[3][1], config[3][2], 5, 5, 1, 1, 2, 2).set_name(name_prefix + \"5x5\"))\n",
    "    conv5.add(ReLU(True).set_name(name_prefix + \"relu_5x5\"))\n",
    "    concat.add(conv5)\n",
    "    \n",
    "    \n",
    "    pool = Sequential()\n",
    "    pool.add(SpatialMaxPooling(3, 3, 1, 1, 1, 1, to_ceil=True).set_name(name_prefix + \"pool\"))\n",
    "    pool.add(SpatialConvolution(input_size, config[4][1], 1, 1, 1, 1).set_name(name_prefix + \"pool_proj\"))\n",
    "    pool.add(ReLU(True).set_name(name_prefix + \"relu_pool_proj\"))\n",
    "    concat.add(pool).set_name(name_prefix + \"output\")\n",
    "    return concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Inception_v1(class_num):\n",
    "    feature1 = Sequential()\n",
    "    \n",
    "    feature1.add(SpatialConvolution(3, 64, 7, 7, 2, 2, 3, 3, 1, False).set_name(\"conv1/7x7_s2\"))\n",
    "    feature1.add(ReLU(True).set_name(\"conv1/relu_7x7\"))\n",
    "    feature1.add(SpatialMaxPooling(3, 3, 2, 2, to_ceil=True).set_name(\"pool1/3x3_s2\"))\n",
    "    feature1.add(SpatialCrossMapLRN(5, 0.0001, 0.75).set_name(\"pool1/norm1\"))\n",
    "    feature1.add(SpatialConvolution(64, 64, 1, 1, 1, 1).set_name(\"conv2/3x3_reduce\"))\n",
    "    feature1.add(ReLU(True).set_name(\"conv2/relu_3x3_reduce\"))\n",
    "    feature1.add(SpatialConvolution(64, 192, 3, 3, 1, 1, 1, 1).set_name(\"conv2/3x3\"))\n",
    "    feature1.add(ReLU(True).set_name(\"conv2/relu_3x3\"))\n",
    "    feature1.add(SpatialCrossMapLRN(5, 0.0001, 0.75).set_name(\"conv2/norm2\"))\n",
    "    feature1.add(SpatialMaxPooling(3, 3, 2, 2, to_ceil=True).set_name(\"pool2/3x3_s2\"))\n",
    "    feature1.add(Inception_Layer_v1(192,scala_T([scala_T([64]), scala_T([96, 128]),scala_T([16, 32]), scala_T([32])]),\n",
    "                                    \"inception_3a/\"))\n",
    "    feature1.add(Inception_Layer_v1(256, scala_T([scala_T([128]), scala_T([128, 192]), scala_T([32, 96]), scala_T([64])]),\n",
    "                                    \"inception_3b/\"))\n",
    "    feature1.add(SpatialMaxPooling(3, 3, 2, 2, to_ceil=True).set_name(\"pool3/3x3_s2\"))\n",
    "    feature1.add(Inception_Layer_v1(480, scala_T([scala_T([192]), scala_T([96, 208]), scala_T([16, 48]), scala_T([64])]),\n",
    "                                    \"inception_4a/\"))\n",
    "\n",
    "    output1 = Sequential()\n",
    "    output1.add(SpatialAveragePooling(5, 5, 3, 3, ceil_mode=True).set_name(\"loss1/ave_pool\"))\n",
    "    output1.add(SpatialConvolution(512, 128, 1, 1, 1, 1).set_name(\"loss1/conv\"))\n",
    "    output1.add(ReLU(True).set_name(\"loss1/relu_conv\"))\n",
    "    output1.add(View([128 * 4 * 4], num_input_dims = 3))\n",
    "    output1.add(Linear(128 * 4 * 4, 1024).set_name(\"loss1/fc\"))\n",
    "    output1.add(ReLU(True).set_name(\"loss1/relu_fc\"))\n",
    "    output1.add(Dropout(0.7).set_name(\"loss1/drop_fc\"))\n",
    "    output1.add(Linear(1024, class_num).set_name(\"loss1/classifier\"))\n",
    "    output1.add(LogSoftMax().set_name(\"loss1/loss\"))\n",
    "\n",
    "    \n",
    "    feature2 = Sequential()\n",
    "    feature2.add(Inception_Layer_v1(512, scala_T([scala_T([160]), scala_T([112, 224]),scala_T([24, 64]), scala_T([64])]),\n",
    "                                    \"inception_4b/\"))\n",
    "    feature2.add(Inception_Layer_v1(512, scala_T([scala_T([128]), scala_T([128, 256]),scala_T([24, 64]), scala_T([64])]),\n",
    "                                    \"inception_4c/\"))\n",
    "    feature2.add(Inception_Layer_v1(512, scala_T([scala_T([112]), scala_T([144, 288]), scala_T([32, 64]), scala_T([64])]),\n",
    "                                    \"inception_4d/\"))\n",
    "\n",
    "    output2 = Sequential()\n",
    "    output2.add(SpatialAveragePooling(5, 5, 3, 3).set_name(\"loss2/ave_pool\"))\n",
    "    output2.add(\n",
    "        SpatialConvolution(528, 128, 1, 1, 1, 1).set_name(\"loss2/conv\"))\n",
    "    output2.add(ReLU(True).set_name(\"loss2/relu_conv\"))\n",
    "    output2.add(View([128 * 4 * 4], num_input_dims=3))\n",
    "    output2.add(Linear(128 * 4 * 4, 1024).set_name(\"loss2/fc\"))\n",
    "    output2.add(ReLU(True).set_name(\"loss2/relu_fc\"))\n",
    "    output2.add(Dropout(0.7).set_name(\"loss2/drop_fc\"))\n",
    "    output2.add(Linear(1024, class_num).set_name(\"loss2/classifier\"))\n",
    "    output2.add(LogSoftMax().set_name(\"loss2/loss\"))\n",
    "\n",
    "    output3 = Sequential()\n",
    "    output3.add(Inception_Layer_v1(528, scala_T([scala_T([256]), scala_T([160, 320]), scala_T([32, 128]), scala_T([128])]),\n",
    "                                   \"inception_4e/\"))\n",
    "    output3.add(SpatialMaxPooling(3, 3, 2, 2, to_ceil=True).set_name(\"pool4/3x3_s2\"))\n",
    "    output3.add(Inception_Layer_v1(832, scala_T([scala_T([256]), scala_T([160, 320]), scala_T([32, 128]), scala_T([128])]),\n",
    "                                   \"inception_5a/\"))\n",
    "    output3.add(Inception_Layer_v1(832,scala_T([scala_T([384]), scala_T([192, 384]),scala_T([48, 128]), scala_T([128])]),\n",
    "                                   \"inception_5b/\"))\n",
    "    output3.add(SpatialAveragePooling(7, 7, 1, 1).set_name(\"pool5/7x7_s1\"))\n",
    "    output3.add(Dropout(0.4).set_name(\"pool5/drop_7x7_s1\"))\n",
    "    output3.add(View([1024], num_input_dims=3))\n",
    "    output3.add(Linear(1024, class_num).set_name(\"loss3/classifier\"))\n",
    "    output3.add(LogSoftMax().set_name(\"loss3/loss3\"))\n",
    "\n",
    "    split2 = Concat(2).set_name(\"split2\")\n",
    "    split2.add(output3)\n",
    "    split2.add(output2)\n",
    "\n",
    "    mainBranch = Sequential()\n",
    "    mainBranch.add(feature2)\n",
    "    mainBranch.add(split2)\n",
    "\n",
    "    split1 = Concat(2).set_name(\"split1\")\n",
    "    split1.add(mainBranch)\n",
    "    split1.add(output1)\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(feature1)\n",
    "    model.add(split1)\n",
    "\n",
    "    model.reset()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_inception_data(folder, file_type=\"image\", data_type=\"train\", normalize=255.0):\n",
    "    \n",
    "#   Getting the path of our data\n",
    "    path = os.path.join(folder, data_type)\n",
    "\n",
    "    if \"seq\" == file_type:\n",
    "#         return imagenet.read_seq_file(sc, path, normalize)\n",
    "        return read_seq_file(sc, path, normalize)\n",
    "    elif \"image\" == file_type:\n",
    "#         return imagenet.read_local(sc, path, normalize)\n",
    "        return read_local(sc, path, normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def config_option_parser():\n",
    "    parser = OptionParser()\n",
    "    parser.add_option(\"-a\", \"--action\", dest=\"action\", default=\"train\")\n",
    "    parser.add_option(\"-f\", \"--folder\", type=str, dest=\"folder\", default=\"\",\n",
    "                      help=\"url of hdfs folder store the hadoop sequence files\")\n",
    "    parser.add_option(\"--model\", type=str, dest=\"model\", default=\"\", help=\"model snapshot location\")\n",
    "    parser.add_option(\"--state\", type=str, dest=\"state\", default=\"\", help=\"state snapshot location\")\n",
    "    parser.add_option(\"--checkpoint\", type=str, dest=\"checkpoint\", default=\"\", help=\"where to cache the model\")\n",
    "    parser.add_option(\"-o\", \"--overwrite\", action=\"store_true\", dest=\"overwrite\", default=False,\n",
    "                      help=\"overwrite checkpoint files\")\n",
    "    parser.add_option(\"-e\", \"--maxEpoch\", type=int, dest=\"maxEpoch\", default=0, help=\"epoch numbers\")\n",
    "    parser.add_option(\"-i\", \"--maxIteration\", type=int, dest=\"maxIteration\", default=62000, help=\"iteration numbers\")\n",
    "    parser.add_option(\"-l\", \"--learningRate\", type=float, dest=\"learningRate\", default=0.01, help=\"iteration numbers\")\n",
    "    parser.add_option(\"-b\", \"--batchSize\", type=int, dest=\"batchSize\", help=\"batch size\")\n",
    "    parser.add_option(\"--classNum\", type=int, dest=\"classNum\", default=1000, help=\"class number\")\n",
    "    parser.add_option(\"--weightDecay\", type=float, dest=\"weightDecay\", default=0.0001, help=\"weight decay\")\n",
    "    parser.add_option(\"--checkpointIteration\", type=int, dest=\"checkpointIteration\", default=620,\n",
    "                      help=\"checkpoint interval of iterations\")\n",
    "\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parser = config_option_parser()\n",
    "(options, args) = parser.parse_args(sys.argv)\n",
    "\n",
    "# if not options.learningRate:\n",
    "#     parser.error(\"-l --learningRate is a mandatory opt\")\n",
    "# if not options.batchSize:\n",
    "#     parser.error(\"-b --batchSize is a mandatory opt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createSpatialCrossMapLRN\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialCrossMapLRN\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createConcat\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createConcat\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createConcat\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialAveragePooling\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createView\n",
      "creating: createLinear\n",
      "creating: createReLU\n",
      "creating: createDropout\n",
      "creating: createLinear\n",
      "creating: createLogSoftMax\n",
      "creating: createSequential\n",
      "creating: createConcat\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createConcat\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createConcat\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialAveragePooling\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createView\n",
      "creating: createLinear\n",
      "creating: createReLU\n",
      "creating: createDropout\n",
      "creating: createLinear\n",
      "creating: createLogSoftMax\n",
      "creating: createSequential\n",
      "creating: createConcat\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createConcat\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createConcat\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSequential\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialAveragePooling\n",
      "creating: createDropout\n",
      "creating: createView\n",
      "creating: createLinear\n",
      "creating: createLogSoftMax\n",
      "creating: createConcat\n",
      "creating: createSequential\n",
      "creating: createConcat\n",
      "creating: createSequential\n"
     ]
    }
   ],
   "source": [
    "#init engine\n",
    "init_engine()\n",
    "\n",
    "# Build the Model\n",
    "inception_model = Inception_v1(options.classNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../sample/train\n",
      "['nonveg', 'veg']\n",
      "[]\n",
      "[('../sample/train/nonveg/n07711080_1284.JPEG', 1), ('../sample/train/nonveg/n07711080_2950.JPEG', 1), ('../sample/train/nonveg/n07711080_757.JPEG', 1), ('../sample/train/veg/n07705931_3672.JPEG', 2), ('../sample/train/veg/n07705931_6350.JPEG', 2), ('../sample/train/veg/n07705931_76.JPEG', 2)]\n",
      "ParallelCollectionRDD[107] at parallelize at PythonRDD.scala:475\n",
      "../sample/val\n",
      "['nonveg', 'veg']\n",
      "[]\n",
      "[('../sample/val/nonveg/n07710616_21595.JPEG', 1), ('../sample/val/nonveg/n07710616_32372.JPEG', 1), ('../sample/val/nonveg/n07710616_52269.JPEG', 1), ('../sample/val/veg/n00017222_17493.JPEG', 2), ('../sample/val/veg/n00017222_17592.JPEG', 2), ('../sample/val/veg/n00017222_5826.JPEG', 2)]\n",
      "ParallelCollectionRDD[108] at parallelize at PythonRDD.scala:475\n",
      "creating: createSeveralIteration\n",
      "creating: createSeveralIteration\n",
      "creating: createMaxIteration\n",
      "creating: createDefault\n",
      "creating: createSGD\n",
      "creating: createClassNLLCriterion\n",
      "creating: createOptimizer\n",
      "creating: createEveryEpoch\n",
      "creating: createTop1Accuracy\n"
     ]
    }
   ],
   "source": [
    "image_size = 224\n",
    "batch_size = 16\n",
    "classNum = 2\n",
    "no_epochs = 2\n",
    "options.action = \"train\"\n",
    "if options.action == \"train\":\n",
    "    \n",
    "    # create dataset\n",
    "    train_transformer = Transformer([Crop(image_size, image_size),\n",
    "                                     Flip(0.5),\n",
    "                                     ChannelNormalizer(0.485, 0.456, 0.406, 0.229, 0.224, 0.225),\n",
    "                                     TransposeToTensor(False)\n",
    "                                     ])\n",
    "    \n",
    "    options.folder = \"../sample/\"\n",
    "#     print options.folder\n",
    "    train_data = get_inception_data(options.folder, \"image\", \"train\").map(\n",
    "        lambda features_label: (train_transformer(features_label[0]), features_label[1])).map(\n",
    "        lambda features_label: Sample.from_ndarray(features_label[0], features_label[1] + 1))\n",
    "        \n",
    "    val_transformer = Transformer([Crop(image_size, image_size, \"center\"),\n",
    "                                   Flip(0.5),\n",
    "                                   ChannelNormalizer(0.485, 0.456, 0.406, 0.229, 0.224, 0.225),\n",
    "                                   TransposeToTensor(False)\n",
    "                                   ])\n",
    "    #val_data = get_inception_data(options.folder, \"seq\", \"val\").map(\n",
    "    val_data = get_inception_data(options.folder, \"image\", \"val\").map(\n",
    "        lambda features_label: (val_transformer(features_label[0]), features_label[1])).map(\n",
    "        lambda features_label: Sample.from_ndarray(features_label[0], features_label[1] + 1))\n",
    "\n",
    "    # TODO: Check stateSnapshot opt\n",
    "\n",
    "    if options.maxEpoch:\n",
    "            {\"learningRate\": options.learningRate,\n",
    "             \"weightDecay\": options.weightDecay,\n",
    "             \"momentum\": 0.9,\n",
    "             \"dampening\": 0.0}\n",
    "            checkpoint_trigger = EveryEpoch()\n",
    "            test_trigger = EveryEpoch()\n",
    "            end_trigger = MaxEpoch(options.maxEpoch)\n",
    "    else:\n",
    "        state = scala_T(\n",
    "            {\"learningRate\": options.learningRate,\n",
    "             \"weightDecay\": options.weightDecay,\n",
    "             \"momentum\": 0.9,\n",
    "             \"dampening\": 0.0})\n",
    "        checkpoint_trigger = SeveralIteration(options.checkpointIteration)\n",
    "        test_trigger = SeveralIteration(options.checkpointIteration)\n",
    "        end_trigger = MaxIteration(10)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = Optimizer(\n",
    "        model=inception_model,\n",
    "        training_rdd=train_data,\n",
    "#         optim_method=\"SGD\",\n",
    "        optim_method = SGD(learningrate=0.01, learningrate_decay=0.0002),\n",
    "#         optim_method=Adam(learningrate=0.002),\n",
    "        criterion=ClassNLLCriterion(),\n",
    "        end_trigger=end_trigger,\n",
    "        batch_size=batch_size,\n",
    "        #state=state\n",
    "        )\n",
    "    \n",
    "    # Optimizer\n",
    "#     optimizer = Optimizer(\n",
    "#                 model=inception_model,\n",
    "#                 training_rdd=train_data,\n",
    "#                 optim_method=Adam(learningrate=0.002),\n",
    "#                 criterion=ClassNLLCriterion(),\n",
    "#                 end_trigger=MaxEpoch(no_epochs),\n",
    "#                 batch_size=batch_size\n",
    "#                 )\n",
    "        \n",
    "    options.batchSize = 64 \n",
    "    \n",
    "    options.checkpoint = \"../Final_MATT/\"\n",
    "    if options.checkpoint:\n",
    "        optimizer.set_checkpoint(checkpoint_trigger, options.checkpoint, options.overwrite)\n",
    "\n",
    "    \"\"\"\n",
    "    optimizer.set_validation(trigger=test_trigger,\n",
    "                             val_rdd=val_data,\n",
    "                             batch_size=options.batchSize,\n",
    "                             #val_method=[\"Top1Accuracy\", \"Top5Accuracy\"])\n",
    "                             val_method=[\"Loss\"]\n",
    "                             val_method=[Top1Accuracy()])\n",
    "    \"\"\"\n",
    "    optimizer.set_validation(batch_size=2048,\n",
    "                            val_rdd=val_data,\n",
    "                            trigger=EveryEpoch(),\n",
    "                            val_method=[Top1Accuracy()])\n",
    "\n",
    "    trained_model = optimizer.optimize()\n",
    "    \n",
    "elif options.action == \"test\":\n",
    "    # Load a pre-trained model and then validate it through top1 accuracy.\n",
    "    test_transformer = Transformer([Crop(image_size, image_size, \"center\"),\n",
    "                                    Flip(0.5),\n",
    "                                    ChannelNormalizer(0.485, 0.456, 0.406, 0.229, 0.224, 0.225),\n",
    "                                    TransposeToTensor(False)\n",
    "                                    ])\n",
    "    test_data = get_inception_data(options.folder, \"seq\", \"val\").map(\n",
    "        lambda features_label: (test_transformer(features_label[0]), features_label[1])).map(\n",
    "        lambda features_label: Sample.from_ndarray(features_label[0], features_label[1] + 1))\n",
    "    model = Model.load(options.model)\n",
    "    results = model.test(test_data, options.batchSize, [\"Top1Accuracy\", \"Top5Accuracy\"])\n",
    "    for result in results:\n",
    "        print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
